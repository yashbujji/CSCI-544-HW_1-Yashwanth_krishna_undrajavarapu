{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/undrajavarapuyashwanthkrishna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: bs4 in /opt/homebrew/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.9/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tsv_file='amazon_reviews_us_Jewelry_v1_00.tsv'\n",
    " \n",
    "# reading given tsv file\n",
    "amazon_data_table=pd.read_csv(tsv_file,sep='\\t',usecols = ['star_rating','review_body'],low_memory=False)\n",
    "amazon_data_table['star_rating'] = pd.to_numeric(amazon_data_table['star_rating'],errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tsv_file='amazon_reviews_us_Jewelry_v1_00.tsv'\n",
    " \n",
    "# reading given tsv file\n",
    "amazon_data_table=pd.read_csv(tsv_file,sep='\\t',usecols = ['star_rating','review_body'],low_memory=False)\n",
    "amazon_data_table['star_rating'] = pd.to_numeric(amazon_data_table['star_rating'],errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 20000 reviews randomly from each rating class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189.68434106046362\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "amazon_data_table_records_5_rating = amazon_data_table[amazon_data_table['star_rating'] == 5].sample(frac = 1).iloc[:20000]\n",
    "amazon_data_table_records_4_rating = amazon_data_table[amazon_data_table['star_rating'] == 4].sample(frac = 1).iloc[:20000]\n",
    "amazon_data_table_records_3_rating = amazon_data_table[amazon_data_table['star_rating'] == 3].sample(frac = 1).iloc[:20000]\n",
    "amazon_data_table_records_2_rating = amazon_data_table[amazon_data_table['star_rating'] == 2].sample(frac = 1).iloc[:20000] \n",
    "amazon_data_table_records_1_rating = amazon_data_table[amazon_data_table['star_rating'] == 1].sample(frac = 1).iloc[:20000]\n",
    "table = pd.concat([amazon_data_table_records_5_rating,amazon_data_table_records_4_rating, amazon_data_table_records_3_rating,amazon_data_table_records_2_rating,amazon_data_table_records_1_rating])\n",
    "final_amazon_table = table.sample(frac=1).reset_index(drop=True)\n",
    "mean_of_review_body = final_amazon_table.review_body.str.len().mean()\n",
    "print(mean_of_review_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189.68434106046362\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "mean_of_review_body = final_amazon_table.review_body.str.len().mean()\n",
    "print(mean_of_review_body)\n",
    "final_amazon_table['review_body'] = final_amazon_table.review_body.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_amazon_table['Review_body'] = final_amazon_table['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "final_amazon_table = final_amazon_table.drop('review_body', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.37979\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "final_amazon_table['Review_body'].apply(lambda x : re.sub(r' +',' ',x))\n",
    "final_amazon_table['Review_body'].apply(lambda x : \" \".join(re.sub('[^A-Za-z]+','', split) for split in x.split()))\n",
    "final_amazon_table['Review_body'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "final_amazon_table['Review_body'] = [BeautifulSoup(text).get_text() for text in final_amazon_table['Review_body'] ]\n",
    "mean_of_review_body = final_amazon_table.Review_body.str.len().mean()\n",
    "print(mean_of_review_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/undrajavarapuyashwanthkrishna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# final_amazon_table = final_amazon_table['Review_body'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "# final_amazon_table.head(100000)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "def remove_the_stop_words_from_the_amazon_data(text):\n",
    "    main_words=word_tokenize(text)\n",
    "    filtered_chars=[w for w in main_words if w not in stopwords.words('english')]\n",
    "    return filtered_chars\n",
    "final_amazon_table.Review_body=final_amazon_table.Review_body.apply(remove_the_stop_words_from_the_amazon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/undrajavarapuyashwanthkrishna/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.15493\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "amazon_word_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "amazon_data_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words_of_amazon_data(text):\n",
    "    lemma = [amazon_data_lemmatizer.lemmatize(w) for w in text]\n",
    "    return ' '.join(lemma)\n",
    "\n",
    "final_amazon_table['Review_body'] = final_amazon_table.Review_body.apply(lemmatize_words_of_amazon_data)\n",
    "\n",
    "mean_of_review_body = final_amazon_table.Review_body.str.len().mean()\n",
    "print(mean_of_review_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf_idf_extraction_of_amazon_data = TfidfVectorizer()\n",
    "x_train_amazon_data,x_test_amazon_data,y_train_amazon_data,y_test_amazon_data = train_test_split(final_amazon_table['Review_body'],final_amazon_table['star_rating'],test_size = 0.2)\n",
    "x_train__amazon_data_tfidf = tf_idf_extraction_of_amazon_data.fit_transform(x_train_amazon_data)\n",
    "x_test__amazon_data_tfidf = tf_idf_extraction_of_amazon_data.transform(x_test_amazon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4860063275736189,0.4961490683229814,0.4910253257929678\n",
      "0.3138888888888889,0.28391959798994976,0.2981530343007915\n",
      "0.2959925005858917,0.31108374384236454,0.3033505464152756\n",
      "0.35984481086323955,0.372396486825596,0.36601307189542487\n",
      "0.5617948717948718,0.5546835443037975,0.5582165605095541\n",
      "0.40350547994130215,0.40364648825693783,0.4033517077828028\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "perceptron_prediction_for_the_dataset = Perceptron()\n",
    "perceptron_prediction_for_the_dataset.fit(x_train__amazon_data_tfidf,y_train_amazon_data)\n",
    "predictions_test_of_amazon_data = perceptron_prediction_for_the_dataset.predict(x_test__amazon_data_tfidf)\n",
    "accuracy_score(predictions_test_of_amazon_data, y_test_amazon_data)\n",
    "precision = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[0]\n",
    "recall = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[1]\n",
    "f1_score = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[2]\n",
    "print(str(precision[0])+\",\"+ str(recall[0])+\",\"+ str(f1_score[0]))\n",
    "print(str(precision[1])+\",\"+ str(recall[1])+\",\"+ str(f1_score[1]))\n",
    "print(str(precision[2])+\",\"+ str(recall[2])+\",\"+ str(f1_score[2]))\n",
    "print(str(precision[3])+\",\"+ str(recall[3])+\",\"+ str(f1_score[3]))\n",
    "print(str(precision[4])+\",\"+ str(recall[4])+\",\"+ str(f1_score[4]))\n",
    "print(str(np.average(precision))+\",\"+ str(np.average(recall))+\",\"+str(np.average(f1_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4860063275736189,0.4961490683229814,0.4910253257929678\n",
      "0.3138888888888889,0.28391959798994976,0.2981530343007915\n",
      "0.2959925005858917,0.31108374384236454,0.3033505464152756\n",
      "0.35984481086323955,0.372396486825596,0.36601307189542487\n",
      "0.5617948717948718,0.5546835443037975,0.5582165605095541\n",
      "0.40350547994130215,0.40364648825693783,0.4033517077828028\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "classification_of_dataset_with_svm = LinearSVC()\n",
    "classification_of_dataset_with_svm.fit(x_train__amazon_data_tfidf, y_train_amazon_data)\n",
    "prediction_with_svm = classification_of_dataset_with_svm.predict(x_test__amazon_data_tfidf)\n",
    "accuracy_score(prediction_with_svm, y_test_amazon_data)\n",
    "precision_svm = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[0]\n",
    "recall_svm = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[1]\n",
    "f1_score_svm = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[2]\n",
    "print(str(precision_svm[0])+\",\"+ str(recall_svm[0])+\",\"+ str(f1_score_svm[0]))\n",
    "print(str(precision_svm[1])+\",\"+ str(recall_svm[1])+\",\"+ str(f1_score_svm[1]))\n",
    "print(str(precision_svm[2])+\",\"+ str(recall_svm[2])+\",\"+ str(f1_score_svm[2]))\n",
    "print(str(precision_svm[3])+\",\"+ str(recall_svm[3])+\",\"+ str(f1_score_svm[3]))\n",
    "print(str(precision_svm[4])+\",\"+ str(recall_svm[4])+\",\"+ str(f1_score_svm[4]))\n",
    "print(str(np.average(precision_svm))+\",\"+ str(np.average(recall_svm))+\",\"+str(np.average(f1_score_svm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4860063275736189,0.4961490683229814,0.4910253257929678\n",
      "0.3138888888888889,0.28391959798994976,0.2981530343007915\n",
      "0.2959925005858917,0.31108374384236454,0.3033505464152756\n",
      "0.35984481086323955,0.372396486825596,0.36601307189542487\n",
      "0.5617948717948718,0.5546835443037975,0.5582165605095541\n",
      "0.40350547994130215,0.40364648825693783,0.4033517077828028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "# instantiate the model (using the default parameters)\n",
    "classification_of_dataset_with_logreg = LogisticRegression(random_state=16,max_iter=100000)\n",
    "classification_of_dataset_with_logreg.fit(x_train__amazon_data_tfidf,y_train_amazon_data)\n",
    "prediction_with_logreg = classification_of_dataset_with_logreg.predict(x_test__amazon_data_tfidf)\n",
    "accuracy_score(prediction_with_logreg, y_test_amazon_data)\n",
    "precision_lg = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[0]\n",
    "recall_lg = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[1]\n",
    "f1_score_lg = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[2]\n",
    "print(str(precision_lg[0])+\",\"+ str(recall_lg[0])+\",\"+ str(f1_score_lg[0]))\n",
    "print(str(precision_lg[1])+\",\"+ str(recall_lg[1])+\",\"+ str(f1_score_lg[1]))\n",
    "print(str(precision_lg[2])+\",\"+ str(recall_lg[2])+\",\"+ str(f1_score_lg[2]))\n",
    "print(str(precision_lg[3])+\",\"+ str(recall_lg[3])+\",\"+ str(f1_score_lg[3]))\n",
    "print(str(precision_lg[4])+\",\"+ str(recall_lg[4])+\",\"+ str(f1_score_lg[4]))\n",
    "print(str(np.average(precision_lg))+\",\"+ str(np.average(recall_lg))+\",\"+str(np.average(f1_score_lg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4860063275736189,0.4961490683229814,0.4910253257929678\n",
      "0.3138888888888889,0.28391959798994976,0.2981530343007915\n",
      "0.2959925005858917,0.31108374384236454,0.3033505464152756\n",
      "0.35984481086323955,0.372396486825596,0.36601307189542487\n",
      "0.5617948717948718,0.5546835443037975,0.5582165605095541\n",
      "0.40350547994130215,0.40364648825693783,0.4033517077828028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "classification_of_dataset_with_naive_bayes_multinomial = MultinomialNB()\n",
    "classification_of_dataset_with_naive_bayes_multinomial.fit(x_train__amazon_data_tfidf, y_train_amazon_data)\n",
    "prediction_with_naive_bayes = classification_of_dataset_with_naive_bayes_multinomial.predict(x_test__amazon_data_tfidf)\n",
    "accuracy_score(prediction_with_naive_bayes, y_test_amazon_data)\n",
    "precision_nb = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[0]\n",
    "recall_nb = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[1]\n",
    "f1_score_nb = precision_recall_fscore_support(y_test_amazon_data, predictions_test_of_amazon_data)[2]\n",
    "print(str(precision_nb[0])+\",\"+ str(recall_nb[0])+\",\"+ str(f1_score_nb[0]))\n",
    "print(str(precision_nb[1])+\",\"+ str(recall_nb[1])+\",\"+ str(f1_score_nb[1]))\n",
    "print(str(precision_nb[2])+\",\"+ str(recall_nb[2])+\",\"+ str(f1_score_nb[2]))\n",
    "print(str(precision_nb[3])+\",\"+ str(recall_nb[3])+\",\"+ str(f1_score_nb[3]))\n",
    "print(str(precision_nb[4])+\",\"+ str(recall_nb[4])+\",\"+ str(f1_score_nb[4]))\n",
    "print(str(np.average(precision_nb))+\",\"+ str(np.average(recall_nb))+\",\"+str(np.average(f1_score_nb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
